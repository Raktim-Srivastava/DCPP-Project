{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRAPE Code 'Unlisted Companies in India' from the .json file created post CRAWL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hw_7AFsaAfd-"
   },
   "outputs": [],
   "source": [
    "# !pip install bs4\n",
    "# !pip install lxml\n",
    "# !pip install requests_html\n",
    "import csv\n",
    "import json\n",
    "import urllib\n",
    "from urllib.request import urlopen, Request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from requests_html import HTMLSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open CSV file to append data, generate dictionary key value pairs (basis index number) in order to append Columns from the website to be scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koY4dsEeBElg"
   },
   "outputs": [],
   "source": [
    "csvfile = open('test.csv', 'w', newline='',encoding=\"utf-8\")\n",
    "\n",
    "make_table = csv.writer(csvfile, delimiter = ';')\n",
    "\n",
    "attribute_to_element = {\n",
    "    \"CIN\": [\"td\"],\n",
    "    \"Company Status\": [\"td\"],\n",
    "    \"RoC\": [\"td\"],\n",
    "    \"Registration Number\": [\"td\"],\n",
    "    \"Company Category\": [\"td\"],\n",
    "    \"Company Subcategory\": [\"td\"],\n",
    "    \"Class of Company\": [\"td\"],\n",
    "    \"Date of Incorporation\": [\"td\"],\n",
    "    \"Age of Company\": [\"td\"],\n",
    "    \"Activity\": [\"td\"],\n",
    "    \"Number of Members\": [\"td\"],\n",
    "    \"Authorized Capital\": [\"td\"],\n",
    "    \"Paid up Capital\": [\"td\"],\n",
    "    \"Date of Last Annual General Meeting\": [\"td\"],\n",
    "    \"Date of Latest Balance Sheet\": [\"td\"],\n",
    "    \"Email ID\" : [\"p\"],\n",
    "    \"Website\" : [\"p\"],\n",
    "    \"Address\" : [\"p\"],\n",
    "    \"Number of Directors\" : [\"table\"],\n",
    "    \"No of Prosecution\" : [\"table\"],\n",
    "    \"Charges/Borrowing Details\" : [\"table\"]\n",
    "    \n",
    "}\n",
    "attribute_indexes = {\n",
    "    \"CIN\": 1,\n",
    "    \"Company Status\": 5,\n",
    "    \"RoC\": 7,\n",
    "    \"Registration Number\": 9,\n",
    "    \"Company Category\": 11,\n",
    "    \"Company Subcategory\": 13,\n",
    "    \"Class of Company\": 15,\n",
    "    \"Date of Incorporation\": 17,\n",
    "    \"Age of Company\": 19,\n",
    "    \"Activity\": 21,\n",
    "    \"Number of Members\": 23,\n",
    "    \"Authorized Capital\": 27,\n",
    "    \"Paid up Capital\": 29,\n",
    "    \"Date of Last Annual General Meeting\": 35,\n",
    "    \"Date of Latest Balance Sheet\": 37,\n",
    "    \"Email ID\" : 61,\n",
    "    \"Website\" : 62,\n",
    "    \"Address\" : 64,\n",
    "    \"Number of Directors\" : 7,\n",
    "    \"No of Prosecution\" : 10,\n",
    "    \"Charges/Borrowing Details\" : 11\n",
    "}\n",
    "all_attributes = [\"Name\", \"URL\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run scraping on a single link by setting test_out = True\n",
    "## Run scraping on a Crawled_URLs.json file by setting test_out = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ndx3YDWxBLEE",
    "outputId": "57ccf513-0492-4b61-86b2-ee6f188439e2"
   },
   "outputs": [],
   "source": [
    "for attribute in attribute_to_element.keys():\n",
    "    all_attributes.append(attribute)\n",
    "\n",
    "make_table.writerow(all_attributes)\n",
    "\n",
    "test_out = False\n",
    "\n",
    "if test_out:\n",
    "    links = ['https://www.zaubacorp.com/company/GHODWADI-MOVIES-PRIVATE-LIMITED/U22222MH2012PTC232389']\n",
    "else:\n",
    "    with open('crawled_URLs.json', 'r+') as f:\n",
    "        links = json.load(f)\n",
    "\n",
    "    print('Number of raw URLs to look at: ' + str(len(links)))\n",
    "\n",
    "agent = \"Safari\"\n",
    "# Reference: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent\n",
    "\n",
    "view_url = True\n",
    "view_count = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Crawler based on Beautiful Soup library to extract content from the desired Web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17aCF4hIBQbo"
   },
   "outputs": [],
   "source": [
    "def add_element(listed, item):\n",
    "    # process and add item to listed\n",
    "    # print(item)\n",
    "    if item:\n",
    "        text = item.text.replace(\"\\n\", \"\")\n",
    "        listed.append(text)\n",
    "    else:\n",
    "        listed.append(\"not_available\")\n",
    "\n",
    "    return listed\n",
    "\n",
    "def handle_element(soup, element, key):\n",
    "    return soup.find_all(element)[attribute_indexes[key]]\n",
    "\n",
    "for url in links:\n",
    "    time.sleep(1)\n",
    "    if 'https://www.zaubacorp.com/company/' not in url:\n",
    "        continue\n",
    "    if view_url:\n",
    "        print('URL: ' + url, flush = True)\n",
    "\n",
    "    try:\n",
    "\n",
    "        html = urlopen(Request(url, headers = {'User-Agent': agent})).read()\n",
    "        print(\"ok\")\n",
    "        # html = requests.get(url).text\n",
    "    \n",
    "    except Exception as E:\n",
    "        print(url + ' not found.', flush = True)\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    listed = []\n",
    "    \n",
    "    # name\n",
    "    setup = soup.find(\"h1\")\n",
    "\n",
    "    # Also checks if it is a valid page\n",
    "    if setup:\n",
    "        listed.append((setup.text).strip())\n",
    "    else:\n",
    "        # Not a valid page\n",
    "        continue\n",
    "    \n",
    "    # url\n",
    "    listed.append(url)\n",
    "\n",
    "    ctr2 = 0\n",
    "    for key in attribute_to_element.keys():\n",
    "\n",
    "        element = attribute_to_element[key]\n",
    "        setup = handle_element(soup, element, key)\n",
    "        \n",
    "        ctr1 = 0\n",
    "        \n",
    "        \n",
    "        if key== \"Number of Directors\":\n",
    "            links = setup.find_all(\"a\")\n",
    "            for l in links:\n",
    "              if l.text.strip() == \"View other directorships\":\n",
    "                ctr2+=1\n",
    "            # ctr2 = len(setup.find_all(\"a\"))//6\n",
    "            listed.append(str(ctr2))\n",
    "            #print(ctr2)\n",
    "        elif key== \"No of Prosecution\":\n",
    "            \n",
    "            setup = soup.find_all(element)[attribute_indexes[key]+(2*ctr2)]\n",
    "            prosecution_list = (len(setup.find_all('td'))//6)-1\n",
    "            listed.append(str(prosecution_list))\n",
    "        \n",
    "        \n",
    "        elif key== \"Charges/Borrowing Details\":\n",
    "            \n",
    "            setup = soup.find_all(element)[attribute_indexes[key]+(2*ctr2)]\n",
    "            charges_borrowing = setup.find_all('td')\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                i=12\n",
    "                while True:\n",
    "                                       \n",
    "                    ctr1 += float(charges_borrowing[i].text.replace(\",\",\"\"))\n",
    "                    \n",
    "                    i += 7\n",
    "            except:\n",
    "                listed.append(str(ctr1))\n",
    "            \n",
    "        \n",
    "            \n",
    "        elif key== \"Email ID\" or key==\"Website\":\n",
    "            text = setup.text[setup.text.find(\":\")+2:]\n",
    "            text = text.replace(\"\\n\", \"\")\n",
    "            listed.append(text)\n",
    "            \n",
    "        else:\n",
    "            listed = add_element(listed, setup)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "    \n",
    "        make_table.writerow(listed)\n",
    "        csvfile.flush()\n",
    "        print(listed)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Excel and .Json uncleaned output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRwntT-UJ12w"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "# Reading the csv file\n",
    "df_new = pd.read_csv('test.csv', sep = ';')\n",
    " \n",
    "# saving xlsx file\n",
    "xlfile = pd.ExcelWriter('Companies_final.xls')\n",
    "df_new.to_excel(xlfile, index=False)\n",
    "xlfile.save()\n",
    "xlfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_0dA9suvb1e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to convert a CSV to JSON\n",
    "# Takes the file paths as arguments\n",
    "def make_json(csvFilePath, jsonFilePath):\n",
    "     \n",
    "    # create a dictionary\n",
    "    data = {}\n",
    "     \n",
    "    # Open a csv reader called DictReader\n",
    "    with open(csvFilePath, encoding='utf-8') as csvf:\n",
    "        csvReader = csv.DictReader(csvf, delimiter=';')\n",
    "         \n",
    "        # Convert each row into a dictionary\n",
    "        # and add it to data\n",
    "        for rows in csvReader:\n",
    "             \n",
    "            # Assuming a column named 'CIN' to be the primary key\n",
    "            \n",
    "            #print(rows)\n",
    "            \n",
    "            key = rows['CIN']\n",
    "            data[key] = rows\n",
    " \n",
    "    # Open a json writer, and use the json.dumps()\n",
    "    # function to dump data\n",
    "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf:\n",
    "        jsonf.write(json.dumps(data, indent=4))\n",
    "        \n",
    "make_json('test.csv','final_uncleaned.json')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Scraping_Unlisted_final - Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
